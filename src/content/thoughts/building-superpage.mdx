---
title: "Building Superpage"
summary: "Putting Atomic Programming to the test by building a real application — a Hacker News monitor that catches stories I'd otherwise miss."
maturity: "growing"
createdAt: 2025-12-13
updatedAt: 2025-12-26
relatedThoughts: ["atomic-programming", "web-primitives", "hypercompetence"]
---

## Introduction

In an earlier post I described [Atomic Programming](/thoughts/atomic-programming) which is a novel approach to building software systems. The idea is to decompose systems into quite small components called atoms, each of which is easily and quickly buildable by the LLM with a low error rate. This is based on the general observation that LLMs are dramatically more effective when we keep their cognitive load low — what I call [hypercompetence](/thoughts/hypercompetence). Atomic Programming tries to keep the LLM in its hypercompetent "golden zone" all the time, so while I still need to make product and architectural decisions, the time spent on implementation becomes vanishingly small.

The purpose of this post is to walk through actually creating a system using the Atomic Programming approach. This is a "real" system meaning it addresses a need I am actually experiencing in real life, and I feel that if I can build the system I have in mind it will address that need sufficiently well that I would use it regularly — on a daily basis.

## Superpage

The system is called Superpage. Its purpose is to make me a more efficient consumer of Hacker News content. I visit the HN homepage most days and get a huge amount of value from it — both the articles and the discussions. But I know there's more value to be had if I spent more time on the site. At the same time, I don't want to spend more time on the site — or more accurately, I don't want to spend less time doing other things. The vision for Superpage is to be a kind of "time multiplier": half an hour spent on Superpage will be like spending several hours on Hacker News itself.

The MVP addresses the stories I flat out miss. In between my daily visits, stories resonate with the community, accumulate points, rise to the front page, spark interesting discussions, and then descend and disappear — all before I check in again. More than once I've Googled something and found a Hacker News thread with hundreds of comments from just days ago — it had come and gone between my daily visits.

Superpage addresses this by widening the aperture. It monitors the front page frequently so I don't have to, stores what it finds, and gives me tools to sort, filter, and search through everything that's come and gone since my last visit.

<figure>
  <img src="/superpage-context.png" alt="Superpage context diagram" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">Superpage monitors HN frequently so you don't have to.</figcaption>
</figure>

## The Build

Going in, I treated Atomic Programming as a naive but interesting vision. Now I would see what happens when that vision meets reality. Reality is always more complex than what we imagine. This journey would show me which parts of Atomic Programming are strong and which need to evolve. And that's exactly what happened — which is not to say the journey is finished. It's more like going from looking at a map and imagining the trip, to actually journeying that first leg.

I started with a small "Starting Point" — a simplified version of the MVP that was easy to build but gave me something real to work with. From there I iterated four times, with the system usable at the end of each iteration. Below I describe each step in some detail, with expandable sections for those who want to dig deeper. After each step, I share what I learned and how it informed what came next. At the end there are some overall reflections, and a link to all the prompts and source code.

## Starting Point

I started with something simpler than the full MVP — easy to build, but a reasonable foundation to iterate from.

<figure>
  <img src="/superpage-starting-point.png" alt="Starting Point architecture" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">Five atoms, each named for its role in the system.</figcaption>
</figure>

**Rate Limiter** fetches HTML from Hacker News while enforcing a minimum interval between requests — keeping us a good citizen of the internet. **Parser** pulls pages from Rate Limiter, extracts the story data (headline, URL, points, comments, rank, age, etc.), and returns structured JSON. **Cacher** stores the parsed data in memory with a TTL, so we're not hitting Hacker News on every request. **Querier** applies filters and sorts to the cached stories — filter by points, by age, by number of comments. **WebUI** serves a web page that talks to the Querier — the only atom without a REST API, just HTML, CSS, and JavaScript, with a Go server proxying requests to the backend.

The Cacher was silly in retrospect. I was trying to avoid database complexity while still storing content locally — a reasonable instinct, but the TTL approach was flawed in a way I realized even before using it. As the atoms came together, I could see what would happen: the cache refresh had no relationship to what the user was doing. Change a filter, and depending on timing, the system might pull fresh content from Hacker News. The UI would go slow, and worse, show you different stories than what you'd been looking at. You'd ask to filter the content you were seeing, but the filter would apply to new content that arrived because of the TTL. That became the first thing to fix.

The UI was clunky too, but that was fine. I'd pointed Claude at the Querier's API and said "build an appropriate UI for that" — maybe ten minutes of effort. What I got was functional but unoptimized: too many clicks, wasted vertical space, unnecessary labels. It served as a placeholder, though, which was exactly what I needed. It let me interact with the system and exercise its features while I focused on evolving the backend. The real UI work would come later.

None of this bothered me. I knew the Starting Point would be flawed, and given how rapidly it came together, flaws were acceptable. Having actual running code is always a great reality check — I was getting firsthand experiential knowledge in a couple of hours. And I knew I could rebuild parts or all of it just as rapidly.

<details class="detail">
<summary>The Prompts — what they specify, what they leave open, and why minimal works.</summary>

Here are the full prompts for Rate Limiter and Parser. The other atoms follow the same structure.

**Rate Limiter:**

<pre style="white-space: pre-wrap; background: #f5f5f5; padding: 1rem; border-radius: 4px; font-size: 0.9em;">
This is a new greenfield project.
We will build this in Go.
It is called Rate Limiter.
The executable will be called "ratelimiter".

The purpose of the Rate Limiter is to fetch HTML docs from URLs, but to rate limit itself so as to not be a burden upon the remote website.

The Rate Limiter will have two required command line arguments. --rate &lt;num-sec&gt; specifies a number of seconds as a positive integer. The semantics are that the Rate Limiter will make at most one HTTP request every &lt;num-sec&gt; seconds. --api &lt;port-no&gt; specifies the port number where the Rate Limiter's REST API can be accessed.

The REST API will have an endpoint POST /fetch which causes the Rate Limiter to fetch a URL. The URL to be fetched is specified in the request body. The response body contains the HTML document retrieved from that URL. The request and response bodies are both JSON objects. If a request arrives too soon vis-a-vis the rate limit, then the request will block until the Rate Limiter is able to provide a response.

The REST API has another endpoint GET /doc which returns detailed documentation of the entire REST API, including example requests and responses. The documentation does not need to concern itself with system internals or how to operate the system – it is only for clients of the REST API.

When you are done coding make sure the system builds and runs correctly, then write a detailed README in case another developer needs to debug or enhance this system in the future.
</pre>

**Parser:**

<pre style="white-space: pre-wrap; background: #f5f5f5; padding: 1rem; border-radius: 4px; font-size: 0.9em;">
This is a new greenfield project.
We will build this in Go.
It is called Parser.
The executable will be called "parser".

The Parser is part of a larger system. It depends upon a component called the Rate Limiter. Use curl to GET /doc from localhost:8080. That is the documentation for the API of the Rate Limiter.

The role of the Parser is to obtain the current top stories from Hacker News and parse them into structured data. It will do this in response to client requests to its REST API. The Parser will not interact with Hacker News directly, rather it will use the Rate Limiter to obtain the HTML documents for Hacker News URLs: https://news.ycombinator.com/, https://news.ycombinator.com/?p=2, https://news.ycombinator.com/?p=3, and so forth. The number of pages it pulls from Hacker News is determined by a required command line parameter. It will then parse the content from those pages and return that information to the client as a single JSON object.

The Parser will have three required command line arguments. --api &lt;port-no&gt; determines which port number the Parser listens for HTTP requests to its REST API. --ratelimiter tells the Parser which localhost port the Rate Limiter is listening on. --num-pages &lt;N&gt; tells the Parser how many Hacker News Pages to pull (must be a positive integer).

The REST API will have an endpoint POST /fetch through which clients will obtain the Hacker News content. When a request arrives from the client, the Parser will request the N URLs from the Rate Limiter sequentially – no need for concurrency. When the Rate Limiter has the HTML documents it will parse them and construct the JSON object to return to the client. The object will have some top-level metadata about the N documents (when they were fetched, and so forth), as well as an array of stories. For each story there is a JSON object with fields for the headline, the URL of the article, the username of the submitter, the number of points, the number of comments, the URL of the discussion page, the story id (HN's identifier – so that we can track stories over time if we like), the story's current rank, and the story's page as two fields: the units (hours, days, etc.) and the age measured in those units (this mirrors the information HN displays in its web pages).

The REST API has another endpoint GET /doc which returns detailed documentation of the entire REST API, including example requests and responses. The documentation does not need to concern itself with system internals or how to operate the system – it is only for clients of the REST API.

When you are done coding make sure the system builds and runs correctly, then write a detailed README in case another developer needs to debug or enhance this system in the future. To test the system you will use curl to obtain the structured data from the Parser and then also use curl to pull the actual HTML from Hacker News and make sure the two align.
</pre>

**What these prompts share:** The first few lines are boilerplate — greenfield, Go, name, executable. Then a sentence about the atom's role in the larger system, providing minimal but useful context. Then command line arguments for configuration — port numbers and functional parameters. Then the REST API, which is the core of the atom's definition. Finally, `GET /doc` and instructions to test.

**What they leave open:** Notice what the prompts don't specify: JSON field names, internal structure, error handling details. I describe the capability needed, not how to implement it. The LLM makes reasonable choices, and reasonable is good enough. This is high leverage — short prompts are fast to write and fast to modify. I'm sketching intent and letting the LLM fill in the rest. The less the definition spills into implementation details, the more freedom I have to iterate.

I do specify `POST /fetch` explicitly, but only because otherwise Claude tends to ask what the endpoint should be called. That question is a waste of time.

**The `/doc` endpoint:** Every atom exposes `GET /doc`, returning documentation of its API. This is how atoms discover each other during development. The Parser prompt tells Claude to curl the Rate Limiter's `/doc` endpoint to understand what it's working with. Claude can also call the API directly — blurring the line between understanding the system and testing it.

**Why coupling is fine:** Traditional API design tries to decouple components — hedge against future changes you're only guessing at. With Atomic Programming, the calculus shifts. We can respond to changes when we know what they are, rather than guessing upfront. If three atoms are tightly coupled, throw them all away and rebuild all three. That's not scary when each takes minutes. Coupling makes larger changes more likely, but it also makes those changes cheaper.

</details>

<details class="detail">
<summary>On Speed and Iteration — 45 minutes of implementation, a couple hours total, and the "build many to throw away" mindset.</summary>

Implementation took about 45 minutes across the five atoms — roughly 10 minutes each once the prompt was ready. Including time to think through what to build and write the prompts, maybe two hours total.

There's evident inefficiency in that 45 minutes. After pasting a prompt, Claude formulates a plan and asks clarifying questions. These occasionally surfaced something useful, but often felt performative — like Claude felt obligated to ask questions before starting. I want to lean toward learning by building, not planning.

Other overhead: starting and stopping atoms manually, managing the Procfile, context-switching between Claude and my terminal. Better tooling could automate much of this. I suspect a well-automated flow could cut the time significantly and let me stay hands-off.

I was intentionally reckless. Since implementation is cheap, failed ideas cost less. The minimum bar was a system I could interact with — real enough to learn from, even if not refined enough to use daily.

"Build one to throw away" becomes "build many to throw away." I knew the Starting Point wouldn't be the final system. I knew the next iteration wouldn't be either. That's fine. Each version gets me closer and teaches me something.

</details>

<details class="detail">
<summary>The Starting Point UI — functional but clunky, which was fine for now.</summary>

![Starting Point UI](/superpage-initial-ui.png)

The placeholder UI that Claude generated with minimal guidance. You can see the inefficiencies: every filter control takes up its own row, wasting vertical space that should go to the content. The labels are redundant — "Min Points" next to an input field labeled "Min Points." Every change requires multiple clicks. The sorting controls duplicate the filter layout pattern even though they work differently.

(Note: the "Refresh" button visible here is actually from the next iteration — that's the screenshot I had on hand. In the true Starting Point, the refresh was automatic via the Cacher's TTL, which as discussed was a flawed design.)

</details>

## Iteration 1: Adding Refresh

The fix: remove the Cacher and have the Querier hold content in memory instead. The Querier's API stayed mostly the same, plus a new endpoint to trigger a refresh. When called, it discards its data and pulls fresh content through the Parser — which triggers the Rate Limiter, which hits Hacker News. The WebUI gained a refresh button to invoke this flow. I threw away the Cacher and its prompt, rewrote the Querier and WebUI prompts, and rebuilt both. About two hours including thinking time.

<figure>
  <img src="/superpage-adding-refresh.png" alt="Adding Refresh architecture" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">Blue for less frequent refreshes, red for more frequent filtering and sorting.</figcaption>
</figure>

This was a significant change architecturally — I removed an entire atom and rewrote two others. And it still didn't land me at MVP. The system was better, but I knew it wasn't the version I'd ultimately use. That was fine. Iteration was cheap enough that I didn't need each version to be a milestone.

In a way, I was extending "build one to throw away" into "build many to throw away." I would build as many intermediate versions as I needed, each getting me closer, none needing to justify its existence beyond what I learned from it.

## Iteration 2: Moving to a Database

Persistent storage was the next step — and a big one. With the system harvesting snapshots continuously, I could finally realize the core vision: catch stories that rose and fell between my visits. No more missing that thread with hundreds of comments that came and went while I was away.

I replaced the Querier with ContentDB — a SQLite-backed store — and added a Harvester to periodically pull content from the Parser and write it to the database. The UI needed rethinking too. With ContentDB complete, Claude could inspect its API and query actual data, and from that back-and-forth arose the new model: instead of one list of stories, the user would select a time range and choose a view — highest rank achieved, most points, most comments. The UI moved from "here are the current stories" to "here's what happened in this time window."

<figure>
  <img src="/superpage-database.png" alt="Moving to a Database architecture" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">ContentDB handled both storage and query logic — too much for one atom.</figcaption>
</figure>

It didn't work. ContentDB passed Claude's smoke tests in isolation, but once the full system was running, queries that should have returned stories came back empty. Claude and I tried debugging — more than once Claude declared victory while bugs persisted. I pulled the plug fairly quickly. This wasn't a problem I wanted to solve through debugging.

My intuition: ContentDB was too big. I'd been thinking of it as a somewhat generic data store, which made the API and storage model more abstract — and therefore more complex — than necessary. Too many responsibilities in one atom. I'd pushed past the golden zone.

<details class="detail">
<summary>The API Design Epiphany — why matching the Parser's output exactly was the right move, and what it costs.</summary>

As I discussed ContentDB with Claude, I had it inspect the Parser's API to understand the data shape. Claude suggested the write endpoint could match the Parser's output exactly — same JSON structure, no transformation.

This felt like an epiphany. I'm used to thinking carefully about API design, trying to strike a balance between addressing the problem and keeping things decoupled. But here, the simple, stupid route was better. No abstraction layer, no mapping between shapes. The Harvester just passes data through.

The price is coupling — Parser, Harvester, and ContentDB are now intertwined. Change one, potentially change all three. But that's the Atomic Programming tradeoff: coupling makes larger changes more likely, and also makes them cheaper. Traditional API design hedges against future risks you're only guessing at. Time spent on clean abstractions is time spent navigating nuanced tradeoffs about changes that may never come. Atomic Programming lets you respond when you know what the changes are. That's a much simpler problem. You're not trying to predict the future — you're just rebuilding when the future arrives.

</details>

## Iteration 3: Splitting ContentDB

I broke ContentDB into two simpler atoms. **SnapshotDB** handles persistence only — it stores snapshots to disk and serves them back by time range. No querying logic, no views, just storage and retrieval. I also folded the Harvester into SnapshotDB; a command line parameter sets the snapshot frequency, and SnapshotDB handles the periodic fetches from the Parser itself. One less atom to manage.

**Window Viewer** handles the query logic that ContentDB had tried to combine with storage. Its API takes a time range and a view type — highest rank, most points, most comments — pulls the relevant snapshots from SnapshotDB, and computes the result.

I rebuilt the UI (renamed **Window UI**, thinking I might have additional pages later) and pointed Claude at the Window Viewer's API.

<figure>
  <img src="/superpage-split.png" alt="Splitting ContentDB architecture" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">SnapshotDB absorbed the Harvester; Window Viewer took over query logic.</figcaption>
</figure>

It worked. Two or three hours, and this version of Superpage "just worked." The issue with ContentDB wasn't just size — it was abstraction. I'd been thinking of it as a generic data store, which made the API and storage model more complex than necessary. The new atoms were simpler because they were less abstract — SnapshotDB just stores snapshots, Window Viewer just computes views.

This validated what I'd realized earlier with the API design: sacrificing the reusability of clean abstractions makes sense when building is this fast. You're not paying for insurance against risks you're only guessing at. You're just rebuilding when you know what the changes actually are.

## Iteration 4: Window UI

With the backend stable, I returned to the UI. As noted earlier, Claude had produced something functional but clunky with minimal guidance — too many clicks, wasted vertical space, redundant labels. I'd left it as a placeholder while figuring out the rest of the system. Now it was time to optimize.

Given the level of detail I cared about — whether labels existed, how buttons were arranged, what the interaction felt like — I decided to bypass the prompt and iterate directly with Claude. I would ask for changes, try them, adjust. Claude offered suggestions of its own. The loop from idea to implementation to testing was fast enough to stay in one train of thought.

<figure>
  <img src="/superpage-final-ui.png" alt="Final Window UI" />
  <figcaption style="text-align: center; font-size: 0.9em; color: #666; font-style: italic; margin-top: 0.5rem;">The optimized UI — controls consolidated, labels removed, space reclaimed for content.</figcaption>
</figure>

The optimizations covered layout and interaction: consolidating controls into fewer rows, removing redundant labels, reclaiming space for content. The most sophisticated change was the time range selector.

I'd expected to go back and create a prompt once I landed on a design I liked. But by the time iterating was done, that clearly wouldn't work. Capturing all the detail would require something approaching a specification — which defeats the point of minimal prompts. So I let the code become the source of truth. The prompt went away. This is where prompts show their power as leverage points: they work because they're concise. Once they balloon into detailed specifications, the leverage disappears. For now, the Window UI lives as code. Whether this holds up long-term — whether I can keep pulling it into context as it grows — I don't know.

<details class="detail">
<summary>Time Range Selector — Fibonacci-scaled buttons with a stateful two-click interaction.</summary>

I wanted Fibonacci-scaled intervals (1h, 2h, 3h, 5h, 8h...) as a single row of buttons, with two always selected to mark start and end of the range. This created a UX puzzle: when you click a new button, is it the new start or the new end?

The solution was a stateful two-click interaction. Initially two buttons are selected showing the current range. Click any button and it highlights differently — pulsing to signal the system is waiting for a second click. Click another button and those two become the new range. If you don't click within a few seconds, the animation stops and it resets.

<img src="/timeselector.gif" alt="Time range selector interaction" style="max-width: 100%; border-radius: 4px; margin: 1rem 0;" />

This emerged through iteration. The first implementation had the puzzle but no solution — clicking a button produced unpredictable behavior. Claude and I tried several approaches before landing on the stateful model. The animation was Claude's suggestion.

</details>

## Overall Reflections

Looking back, a layered architecture emerged naturally. Rate Limiter and Parser were defined once and never changed — basic capabilities that were well understood from the start. The middle tier evolved significantly but stayed within the AP approach. The UI was different: once I moved past placeholder mode, I needed direct control over details in a way that didn't fit the prompt-and-rebuild pattern.

I'm curious what happens next. The testing has been rudimentary — smoke tests by Claude, some manual poking. As I run this daily, will corner cases emerge? In principle they're easy to fix, but there's requirements management involved: making sure rebuilt atoms don't replicate old bugs. I don't yet know where that information lives — in the prompts? A separate tracker?

<div class="expand-toggle reflection"><button>Expand All Reflections</button></div>

<details class="reflection">
<summary>On Audience of One</summary>

The UI choices I made were personal. Having an audience of one made optimization easier — I only needed to please myself. That's very different from designing for millions of users.

Maybe this hints at where software is going. If building is cheap enough, "product" becomes less about designing for the average user and more about empowering people to craft interfaces for themselves. An audience of one might be the right scale.

</details>

<details class="reflection">
<summary>Does This Challenge the Premise?</summary>

If bugs emerge at scale, does that undermine Atomic Programming's premise — that atoms are small and simple enough for the LLM to implement with few mistakes?

Not necessarily. Bugs are inevitable in any approach. The question is whether they're manageable. If I can track issues and rebuild atoms quickly, the premise holds. If I end up in an endless whack-a-mole cycle, it doesn't.

I'll find out.

</details>

## Appendix: Prompts and Source Code

The prompts and source code are on [GitHub](https://github.com/rphilander/superpage-phase1). Looking back, each iteration should have been a commit — but I only thought to share this after the journey was underway. The repo reflects the final state.
